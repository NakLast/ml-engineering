version: "3.9"

services:
  sglang:
    image: lmsysorg/sglang:latest
    container_name: sglang-gemma4b-main
    shm_size: 32g
    ports:
      - "30000:30000"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    environment:
      - HUGGINGFACE_HUB_TOKEN=${huggingfacetoken}
    command: >
      python3 -m sglang.launch_server
        --model google/gemma-2-2b-it
        --tp 1
        --trust-remote-code
        --host 0.0.0.0
        --port 30000
        --mem-fraction-static 0.9
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    ipc: host
    networks:
      - appnet

  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: backend
    ports:
      - "3001:3001"
    # volumes:
    #   - ./backend:/app
    working_dir: /app
    command: ["pnpm", "run", "dev"]
    depends_on:
      - sglang
    environment:
      - SGLANG_API_URL=http://sglang:30000/v1/chat/completions
    restart: unless-stopped
    networks:
      - appnet

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    container_name: frontend
    ports:
      - "5173:5173"
    working_dir: /app
    command: ["npm", "run", "dev", "--", "--host"]
    depends_on:
      - backend
    restart: unless-stopped
    networks:
      - appnet

networks:
  appnet:
    driver: bridge