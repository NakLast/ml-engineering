version: "3.9"

services:
  vllm:
    image: vllm/vllm:latest # Replace with actual image or custom-built one
    container_name: vllm
    ports:
      - "8000:8000"
    volumes:
      - ./vllm:/app
    working_dir: /app
    command:
      [
        "python3",
        "-m",
        "vllm.entrypoints.openai.api_server",
        "--host",
        "0.0.0.0",
      ]
    restart: unless-stopped

  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: backend
    ports:
      - "3001:3001"
    volumes:
      - ./backend:/app
    working_dir: /app
    command: ["npm", "run", "dev"]
    depends_on:
      - vllm
    environment:
      - VLLM_API_URL=http://vllm:8000
    restart: unless-stopped

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    container_name: frontend
    ports:
      - "5173:5173"
    volumes:
      - ./frontend:/app
    working_dir: /app
    command: ["npm", "run", "dev", "--", "--host"]
    depends_on:
      - backend
    restart: unless-stopped
